from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import torch
from diffusers import StableDiffusionPipeline
from io import BytesIO
import logging
from typing import Optional
import os
import uuid
from minio import Minio
import json
from minio.error import S3Error

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="Stable Diffusion API", version="1.0.0")

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
pipeline: Optional[StableDiffusionPipeline] = None
minio_client: Optional[Minio] = None

class GenerateRequest(BaseModel):
    prompt: str
    width: int = 512
    height: int = 512
    num_inference_steps: int = 20
    guidance_scale: float = 7.5

@app.on_event("startup")
async def load_model():
    """–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∏ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º MinIO –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ"""
    global pipeline, minio_client
    
    try:
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ MinIO
        logger.info("üóÑÔ∏è –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º MinIO...")
        minio_client = Minio(
            "minio:9000",
            access_key="minioadmin",
            secret_key="minioadmin",
            secure=False
        )
        
        # –°–æ–∑–¥–∞–µ–º bucket –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        bucket_name = "ai-images"
        if not minio_client.bucket_exists(bucket_name):
            minio_client.make_bucket(bucket_name)
            logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω bucket: {bucket_name}")

        public_policy = {
            "Version": "2012-10-17",
            "Statement": [
                {
                    "Effect": "Allow",
                    "Principal": {"AWS": ["*"]},
                    "Action": ["s3:GetObject"],
                    "Resource": [f"arn:aws:s3:::{bucket_name}/*"]
                }
            ]
        }

        try:
            minio_client.set_bucket_policy(bucket_name, json.dumps(public_policy))
            logger.info(f"üåç –ü—É–±–ª–∏—á–Ω–∞—è –ø–æ–ª–∏—Ç–∏–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –¥–ª—è {bucket_name}")
        except S3Error as e:
            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –ø—É–±–ª–∏—á–Ω—É—é –ø–æ–ª–∏—Ç–∏–∫—É: {e}")
        
        logger.info("üöÄ –ó–∞–≥—Ä—É–∂–∞–µ–º Stable Diffusion –º–æ–¥–µ–ª—å...")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
        if torch.cuda.is_available():
            device = "cuda"
            torch_dtype = torch.float16
            logger.info("‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º CUDA")
        else:
            device = "cpu"
            torch_dtype = torch.float32
            logger.info("‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ–º CPU (–±—É–¥–µ—Ç –º–µ–¥–ª–µ–Ω–Ω–æ)")
        
        # –ü—É—Ç—å –∫ –º–æ–¥–µ–ª–∏ –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏–ª–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        model_id = os.getenv("HF_MODEL_REPO", "runwayml/stable-diffusion-v1-5")
        model_path = os.getenv("MODEL_PATH", None)
        
        logger.info(f"üìÇ –ú–æ–¥–µ–ª—å: {model_path if model_path else model_id}")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
        if model_path and os.path.exists(model_path) and os.path.exists(os.path.join(model_path, "model_index.json")):
            # –ï—Å–ª–∏ –µ—Å—Ç—å –ª–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π
            pipeline = StableDiffusionPipeline.from_pretrained(
                model_path,
                torch_dtype=torch_dtype,
                use_safetensors=True,
                local_files_only=True
            )
            logger.info("üìÅ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –ª–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å")
        else:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–∑ HuggingFace
            logger.info(f"üì• –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∏–∑ HuggingFace: {model_id}")
            pipeline = StableDiffusionPipeline.from_pretrained(
                model_id,
                torch_dtype=torch_dtype,
                use_safetensors=True
            )
            logger.info("üåê –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –º–æ–¥–µ–ª—å –∏–∑ HuggingFace")
        
        # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
        pipeline = pipeline.to(device)
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
        if device == "cuda":
            pipeline.enable_attention_slicing()
            try:
                pipeline.enable_xformers_memory_efficient_attention()
                logger.info("‚ú® –í–∫–ª—é—á–µ–Ω–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è xformers")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è xformers –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
        else:
            # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è CPU
            pipeline.enable_attention_slicing()
        
        logger.info("üéâ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏ –≥–æ—Ç–æ–≤–∞ –∫ —Ä–∞–±–æ—Ç–µ!")
        
        # –¢–µ—Å—Ç–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–ª—è –ø—Ä–æ–≥—Ä–µ–≤–∞
        logger.info("üî• –ü—Ä–æ–≥—Ä–µ–≤–∞–µ–º –º–æ–¥–µ–ª—å...")
        with torch.no_grad():
            _ = pipeline("test", num_inference_steps=1, guidance_scale=1.0, width=64, height=64)
        logger.info("‚úÖ –ú–æ–¥–µ–ª—å –ø—Ä–æ–≥—Ä–µ—Ç–∞!")
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
        raise e

@app.get("/health")
async def health_check():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ —Å–µ—Ä–≤–∏—Å–∞"""
    if pipeline is None:
        raise HTTPException(status_code=503, detail="–ú–æ–¥–µ–ª—å –µ—â–µ –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è")
    
    return {
        "status": "healthy",
        "model_loaded": True,
        "device": "cuda" if torch.cuda.is_available() else "cpu",
        "model": os.getenv("HF_MODEL_REPO", "runwayml/stable-diffusion-v1-5"),
        "torch_version": torch.__version__
    }

@app.post("/generate")
async def generate_image(request: GenerateRequest):
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –≤ MinIO - –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç URL"""
    if pipeline is None:
        raise HTTPException(status_code=503, detail="–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
    
    try:
        logger.info(f"üé® –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º: '{request.prompt}' ({request.width}x{request.height})")
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        if request.width > 1024 or request.height > 1024:
            raise HTTPException(status_code=400, detail="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ 1024x1024")
        
        if request.num_inference_steps > 50:
            raise HTTPException(status_code=400, detail="–ú–∞–∫—Å–∏–º—É–º 50 —à–∞–≥–æ–≤")
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        device = next(pipeline.unet.parameters()).device
        
        with torch.autocast(device.type if device.type != "cpu" else "cpu"):
            result = pipeline(
                prompt=request.prompt,
                width=request.width,
                height=request.height,
                num_inference_steps=request.num_inference_steps,
                guidance_scale=request.guidance_scale,
                generator=torch.Generator(device=device).manual_seed(42)
            )
        
        image = result.images[0]
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ –±–∞–π—Ç—ã –¥–ª—è MinIO
        buffer = BytesIO()
        image.save(buffer, format="PNG", optimize=True)
        image_bytes = buffer.getvalue()
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —É–Ω–∏–∫–∞–ª—å–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞
        filename = f"generated_{uuid.uuid4().hex[:8]}.png"
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤ MinIO
        minio_client.put_object(
            "ai-images",
            filename,
            BytesIO(image_bytes),
            length=len(image_bytes),
            content_type="image/png"
        )
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—É–±–ª–∏—á–Ω—ã–π URL
        public_host = "http://localhost:9000".rstrip("/")
        image_url = f"{public_host}/ai-images/{filename}" 
        
        logger.info(f"‚úÖ –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≥–æ—Ç–æ–≤–æ! URL: {image_url}")
        
        return {
            "url": image_url,  # ‚Üê –≠—Ç–æ –æ–∂–∏–¥–∞–µ—Ç –≤–∞—à Java —Å–µ—Ä–≤–∏—Å
            "prompt": request.prompt,
            "width": request.width,
            "height": request.height,
            "steps": request.num_inference_steps,
            "guidance": request.guidance_scale
        }
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {str(e)}")

@app.get("/")
async def root():
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± API"""
    return {
        "service": "Stable Diffusion API",
        "version": "1.0.0",
        "model": os.getenv("HF_MODEL_REPO", "runwayml/stable-diffusion-v1-5"),
        "endpoints": {
            "health": "GET /health - –ü—Ä–æ–≤–µ—Ä–∫–∞ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏",
            "generate": "POST /generate - –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è",
            "docs": "GET /docs - Swagger –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è"
        },
        "status": "ready" if pipeline else "loading"
    }

@app.get("/status")
async def get_status():
    """–î–µ—Ç–∞–ª—å–Ω—ã–π —Å—Ç–∞—Ç—É—Å —Å–µ—Ä–≤–∏—Å–∞"""
    return {
        "model_loaded": pipeline is not None,
        "cuda_available": torch.cuda.is_available(),
        "device": "cuda" if torch.cuda.is_available() else "cpu",
        "torch_version": torch.__version__,
        "model_path": os.getenv("MODEL_PATH", "HuggingFace"),
        "model_repo": os.getenv("HF_MODEL_REPO", "runwayml/stable-diffusion-v1-5")
    }

if __name__ == "__main__":
    import uvicorn
    logger.info("üöÄ –ó–∞–ø—É—Å–∫–∞–µ–º Stable Diffusion API...")
    uvicorn.run(app, host="0.0.0.0", port=8000, log_level="info")
